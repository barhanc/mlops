{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c38bbe7d",
   "metadata": {},
   "source": [
    "## SQLAlchemy and vector search (2 points)\n",
    "\n",
    "### SQLAlchemy setup\n",
    "\n",
    "For defining the table, we will use Python and [SQLAlchemy\n",
    "framework](https://www.sqlalchemy.org/).\n",
    "\n",
    "A good practice is to build the database URL using the SQLAlchemy library. This\n",
    "option is much more readable, safer, and easier to maintain than using the\n",
    "connection string directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9da05d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.engine import URL\n",
    "\n",
    "db_url = URL.create(\n",
    "    drivername=\"postgresql+psycopg\",\n",
    "    username=\"postgres\",\n",
    "    password=\"password\",\n",
    "    host=\"localhost\",\n",
    "    port=5555,\n",
    "    database=\"similarity_search_service_db\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a77376e",
   "metadata": {},
   "source": [
    "Tables in SQLAlchemy are defined using class-based design. Typically,\n",
    "application defines a single `Base` class, from which concrete tables inherit.\n",
    "They are defined quite similarly to Pydantic, i.e. with attributes and types.\n",
    "However, here we also need to assign concrete database types. To integrate it\n",
    "with vector search, we will also use `pgvector` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6472629",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pgvector.sqlalchemy import Vector\n",
    "from sqlalchemy import Integer, String\n",
    "from sqlalchemy.orm import DeclarativeBase, Mapped, mapped_column\n",
    "\n",
    "\n",
    "# Create the base class for the table definition\n",
    "class Base(DeclarativeBase):\n",
    "    __abstract__ = True\n",
    "\n",
    "\n",
    "# Create the table definition\n",
    "class Images(Base):\n",
    "    __tablename__ = \"images\"\n",
    "    VECTOR_LENGTH = 512\n",
    "\n",
    "    # primary key\n",
    "    id: Mapped[int] = mapped_column(Integer, primary_key=True)\n",
    "\n",
    "    # image path - we will use it to store the path to the image file, after\n",
    "    # similarity search we can use it to retrieve the image and display it\n",
    "    image_path: Mapped[str] = mapped_column(String(256))\n",
    "\n",
    "    # image embedding - we will store the image embedding in this column, the\n",
    "    # image embedding is a list of 512 floats this is the output of the sentence\n",
    "    # transformer model\n",
    "    image_embedding: Mapped[List[float]] = mapped_column(Vector(VECTOR_LENGTH))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ecb2c0",
   "metadata": {},
   "source": [
    "To actually connect to the database, interact with it and run queries, we use\n",
    "engine object. It is created with `create_engine()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d62b98a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "engine = create_engine(db_url)\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(\"CREATE EXTENSION IF NOT EXISTS vector;\"))\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5134fa",
   "metadata": {},
   "source": [
    "Now we can create the table in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e21c0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Base.metadata.create_all(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5155581a",
   "metadata": {},
   "source": [
    "### SQLAlchemy queries\n",
    "\n",
    "For testing, we need to insert some data into the table. Code below has one\n",
    "thing missing - you need to create the `Images` object based on provided data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "132be991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "import numpy as np\n",
    "\n",
    "from sqlalchemy import select\n",
    "from sqlalchemy.orm import Session\n",
    "\n",
    "\n",
    "# reusable function to insert data into the table\n",
    "def insert_image(engine: sqlalchemy.Engine, image_path: str, image_embedding: list[float]):\n",
    "    with Session(engine) as session:\n",
    "        # create the image object\n",
    "        image = Images(image_path=image_path, image_embedding=image_embedding)\n",
    "        # add the image object to the session\n",
    "        session.add(image)\n",
    "        # commit the transaction\n",
    "        session.commit()\n",
    "\n",
    "\n",
    "# calculate the cosine similarity between the first image and the K rest of the\n",
    "# images, order the images by the similarity score\n",
    "def find_k_images(engine: sqlalchemy.Engine, k: int, orginal_image: Images) -> list[Images]:\n",
    "    with Session(engine) as session:\n",
    "        # execution_options={\"prebuffer_rows\": True} is used to prebuffer the\n",
    "        # rows, this is useful when we want to fetch the rows in chunks and\n",
    "        # return them after session is closed\n",
    "        result = session.execute(\n",
    "            (select(Images).order_by(Images.image_embedding.cosine_distance(orginal_image.image_embedding)).limit(k)),\n",
    "            execution_options={\"prebuffer_rows\": True},\n",
    "        )\n",
    "        return result\n",
    "\n",
    "\n",
    "# insert some data into the table\n",
    "N = 100\n",
    "for i in range(N):\n",
    "    image_path = f\"image_{i}.jpg\"\n",
    "    image_embedding = np.random.rand(512).tolist()\n",
    "    insert_image(engine, image_path, image_embedding)\n",
    "\n",
    "# select first image from the table\n",
    "with Session(engine) as session:\n",
    "    image = session.query(Images).first()\n",
    "\n",
    "# find the 10 most similar images to the first image\n",
    "k = 10\n",
    "similar_images = find_k_images(engine, k, image).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50995bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_0.jpg\n",
      "image_2.jpg\n",
      "image_83.jpg\n",
      "image_30.jpg\n",
      "image_47.jpg\n",
      "image_60.jpg\n",
      "image_20.jpg\n",
      "image_70.jpg\n",
      "image_69.jpg\n",
      "image_77.jpg\n"
     ]
    }
   ],
   "source": [
    "for img in similar_images:\n",
    "    print(img[0].image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11294fb",
   "metadata": {},
   "source": [
    "## Filtering\n",
    "\n",
    "For more filtering, we will need an actual dataset with rich metadata to filter\n",
    "by. For this, we will utilize [Steam Games\n",
    "Dataset](https://huggingface.co/datasets/FronkonGames/steam-games-dataset). It\n",
    "is hosted on HuggingFace Hub, and we can download it with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "215a595f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AppID': Value('int64'), 'Name': Value('string'), 'Release date': Value('string'), 'Estimated owners': Value('string'), 'Peak CCU': Value('int64'), 'Required age': Value('int64'), 'Price': Value('float64'), 'DLC count': Value('int64'), 'About the game': Value('string'), 'Supported languages': Value('string'), 'Full audio languages': Value('string'), 'Reviews': Value('string'), 'Header image': Value('string'), 'Website': Value('string'), 'Support url': Value('string'), 'Support email': Value('string'), 'Windows': Value('bool'), 'Mac': Value('bool'), 'Linux': Value('bool'), 'Metacritic score': Value('int64'), 'Metacritic url': Value('string'), 'User score': Value('int64'), 'Positive': Value('int64'), 'Negative': Value('int64'), 'Score rank': Value('float64'), 'Achievements': Value('int64'), 'Recommendations': Value('int64'), 'Notes': Value('string'), 'Average playtime forever': Value('int64'), 'Average playtime two weeks': Value('int64'), 'Median playtime forever': Value('int64'), 'Median playtime two weeks': Value('int64'), 'Developers': Value('string'), 'Publishers': Value('string'), 'Categories': Value('string'), 'Genres': Value('string'), 'Tags': Value('string'), 'Screenshots': Value('string'), 'Movies': Value('string')}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"FronkonGames/steam-games-dataset\")\n",
    "\n",
    "# get columns names and types\n",
    "columns = dataset[\"train\"].features\n",
    "print(columns)\n",
    "\n",
    "columns_to_keep = [\"Name\", \"Windows\", \"Linux\", \"Mac\", \"About the game\", \"Supported languages\", \"Price\"]\n",
    "\n",
    "N = 40_000\n",
    "dataset = dataset[\"train\"].select_columns(columns_to_keep).select(range(N))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824553bc",
   "metadata": {},
   "source": [
    "We will use columns:\n",
    "* `Name`\n",
    "* `About the game`\n",
    "* `Price`\n",
    "* `Platforms` - the platforms on which the game is available; note that there is\n",
    "  separate field for each platform (Windows, Linux, macOS)\n",
    "\n",
    "For vector search, we can use the `About the game` column, which is an arbitrary\n",
    "text description. A great model for this purpose is\n",
    "`distiluse-base-multilingual-cased-v2` from [Sentence\n",
    "Transformers](https://sbert.net/). This model is a multilingual text\n",
    "transformers, and thus it will work well for descriptions of games in languages\n",
    "other than English.\n",
    "\n",
    "The `distiluse-base-multilingual-cased-v2` model supports over 50 languages.\n",
    "This version is a distilled multilingual knowledge model derived from the\n",
    "original Universal Sentence Encoder, which only supported 15 languages. While\n",
    "the v2 model supports a wider range of languages, it is noted that its\n",
    "performance may be a bit lower, compared to the original model for the languages\n",
    "it shares with v1, particularly the first 15 languages that were supported\n",
    "originally. Read the paper if you're interested:\n",
    "https://arxiv.org/pdf/2004.09813.\n",
    "\n",
    "When looking at\n",
    "https://huggingface.co/sentence-transformers/distiluse-base-multilingual-cased-v2,\n",
    "one can see that it produces 512-dimensional embeddings. This is a value we need\n",
    "to declare in the table definition as vector length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a6500bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import Integer, Float, Boolean\n",
    "\n",
    "\n",
    "class Games(Base):\n",
    "    __tablename__ = \"games\"\n",
    "    __table_args__ = {\"extend_existing\": True}\n",
    "\n",
    "    # the vector size produced by the model taken from documentation\n",
    "    # https://huggingface.co/sentence-transformers/distiluse-base-multilingual-cased-v2\n",
    "    VECTOR_LENGTH = 512\n",
    "\n",
    "    id: Mapped[int] = mapped_column(Integer, primary_key=True)\n",
    "    name: Mapped[str] = mapped_column(String(256))\n",
    "    description: Mapped[str] = mapped_column(String(4096))\n",
    "    windows: Mapped[bool] = mapped_column(Boolean)\n",
    "    linux: Mapped[bool] = mapped_column(Boolean)\n",
    "    mac: Mapped[bool] = mapped_column(Boolean)\n",
    "    price: Mapped[float] = mapped_column(Float)\n",
    "    game_description_embedding: Mapped[List[int]] = mapped_column(Vector(VECTOR_LENGTH))\n",
    "\n",
    "\n",
    "Base.metadata.drop_all(engine)\n",
    "Base.metadata.create_all(engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6da212",
   "metadata": {},
   "source": [
    "Let's prepare the function that will generate the embeddings for the games\n",
    "descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cdb96cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "checkpoint = \"distiluse-base-multilingual-cased-v2\"\n",
    "model = SentenceTransformer(checkpoint)\n",
    "\n",
    "\n",
    "def generate_embeddings(text: str) -> list[float]:\n",
    "    return model.encode(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4d601d",
   "metadata": {},
   "source": [
    "Now let's prepare the function that will insert the data into the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ee6bd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def insert_games(engine, dataset):\n",
    "    with tqdm(total=len(dataset)) as pbar:\n",
    "        for i, game in enumerate(dataset):\n",
    "            game_description = game[\"About the game\"] or \"\"\n",
    "            game_embedding = generate_embeddings(game_description)\n",
    "            name, windows, linux, mac, price = game[\"Name\"], game[\"Windows\"], game[\"Linux\"], game[\"Mac\"], game[\"Price\"]\n",
    "            if name and windows and linux and mac and price and game_description:\n",
    "                game = Games(\n",
    "                    name=game[\"Name\"],\n",
    "                    description=game_description[0:4096],\n",
    "                    windows=game[\"Windows\"],\n",
    "                    linux=game[\"Linux\"],\n",
    "                    mac=game[\"Mac\"],\n",
    "                    price=game[\"Price\"],\n",
    "                    game_description_embedding=game_embedding,\n",
    "                )\n",
    "                with Session(engine) as session:\n",
    "                    session.add(game)\n",
    "                    session.commit()\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f1953a",
   "metadata": {},
   "source": [
    "Now we can insert the data into the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f720422",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40000/40000 [08:37<00:00, 77.29it/s] \n"
     ]
    }
   ],
   "source": [
    "insert_games(engine, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7416de17",
   "metadata": {},
   "source": [
    "Now the function that will find the games similar to the given game, and also\n",
    "include given filtering criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aecf05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "\n",
    "def find_game(\n",
    "    engine: sqlalchemy.Engine,\n",
    "    game_description: str,\n",
    "    windows: Optional[bool] = None,\n",
    "    linux: Optional[bool] = None,\n",
    "    mac: Optional[bool] = None,\n",
    "    price: Optional[int] = None,\n",
    "):\n",
    "    with Session(engine) as session:\n",
    "        game_embedding = generate_embeddings(game_description)  # generate game embedding\n",
    "\n",
    "        query = select(Games).order_by(Games.game_description_embedding.cosine_distance(game_embedding))\n",
    "\n",
    "        if price:\n",
    "            query = query.filter(Games.price <= price)\n",
    "        if windows:\n",
    "            query = query.filter(Games.windows)\n",
    "        if linux:\n",
    "            query = query.filter(Games.linux)\n",
    "        if mac:\n",
    "            query = query.filter(Games.mac)\n",
    "\n",
    "        result = session.execute(query, execution_options={\"prebuffer_rows\": True})\n",
    "        game = result.scalars().first()\n",
    "\n",
    "        return game"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39a1502",
   "metadata": {},
   "source": [
    "Our first vector search service is ready to use! Let's check it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bae5f170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game: Ultimate Spider Hero\n",
      "Description: Ultimate Spider Hero game was designed for real heroes! Your mission is to help poor residents of the Metropolis and to save them from the terrible monsters. Move forward to fight your enemies and try not to fall! Features: Simple and addictive gameplay Nice graphics Awesome Ultimate Spider Hero Countless Steam achievements for you to collect! Compatibility with multiple major platforms (Windows, Mac, Linux, SteamOS) Make your way through the endless labyrinths of long, confusing city streets together with your favorite hero from countless movies and cartoons! Although this may look simple enough, things are not as easy as they seem. You will have to learn how to cling into houses properly using your web, otherwise you will fall to your demise. If you manage to do so - you will become a real superhero, armed with elusiveness, agility and speed and the ability to tirelessly swing across the rooftops and between the huge skyscrapers this urban landscape has to offer in this thrilling game of a super spider. It's fun, the visuals are magnificent and the controls are simple!\n",
      "Game: 3D PUZZLE - Modern House\n",
      "Description: Collect a 3D puzzle, transferring things to the right places to create a beautiful house. You need to go to the item, take it by pressing the left mouse button and take the item to the desired location marked in green. If you brought the correct item, it will snap into place and you will receive leaderboard points and achievements for this. Collect as much substance as possible as quickly as possible to get more points for the leaderboard. If you brought the wrong item, you can throw it away, it will return to the starting location so that you can pick it up again.\n"
     ]
    }
   ],
   "source": [
    "game = find_game(engine, \"This is a game about a hero who saves the world\", price=10)\n",
    "print(f\"Game: {game.name}\")\n",
    "print(f\"Description: {game.description}\")\n",
    "\n",
    "game = find_game(engine, game_description=\"Home decorating\", price=20)\n",
    "print(f\"Game: {game.name}\")\n",
    "print(f\"Description: {game.description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b783e0ef",
   "metadata": {},
   "source": [
    "Let's change the filtering requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2fcd825e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game: 3D PUZZLE - Old House\n",
      "Description: Collect a 3D puzzle, transferring things to the right places to create a beautiful house. You need to go to the item, take it by pressing the left mouse button and take the item to the desired location marked in green. If you brought the correct item, it will snap into place and you will receive leaderboard points and achievements for this. Collect as much substance as possible as quickly as possible to get more points for the leaderboard. If you brought the wrong item, you can throw it away, it will return to the starting location so that you can pick it up again.\n"
     ]
    }
   ],
   "source": [
    "game = find_game(engine, game_description=\"Home decorating\", mac=True, price=5)\n",
    "print(f\"Game: {game.name}\")\n",
    "print(f\"Description: {game.description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673f4a31",
   "metadata": {},
   "source": [
    "As you can see, while changing criteria to more strict, results can vary. This\n",
    "is the consequence of a few things: how attribute filtering reduces the results,\n",
    "how model interprets the similarity of descriptions, and how those two things\n",
    "interact together. A major advantage of this approach is its overall simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd2540e",
   "metadata": {},
   "source": [
    "## Retrieval-Augmented Generation (RAG) service (1 point)\n",
    "\n",
    "### Vector database setup\n",
    "\n",
    "In this part of laboratory, we will build a RAG service. It enhances the LLM\n",
    "text generation capabilities with context and information drawn from a knowledge\n",
    "base. Relevant textual information is found with vector search and appended to\n",
    "the prompt, resulting in less hallucinations and more precise, relevant answers.\n",
    "\n",
    "In such cases, we don't relaly need any additional capabilities like attributes\n",
    "filtering, ACID, JOINs or other Postgres-related advantages. Thus, we will use\n",
    "[Milvus](https://milvus.io/), a typical example of vector database. To generate\n",
    "embeddings, we will use [Silver Retriever\n",
    "model](https://huggingface.co/ipipan/silver-retriever-base-v1.1) from Sentence\n",
    "Transformers. It is based on HerBERT model for Polish language, and finetuned\n",
    "for retrieval of similar vectors.\n",
    "\n",
    "Let's connect to the database. Milvus provides its own `pymilvus` library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c26625d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import MilvusClient\n",
    "\n",
    "host = \"localhost\"\n",
    "port = \"19530\"\n",
    "\n",
    "milvus_client = MilvusClient(host=host, port=port)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e007f27e",
   "metadata": {},
   "source": [
    "Vector databases work quite similarly to document databases like e.g. MongoDB.\n",
    "We define not a table, but a **collection** with specific **schema**, but\n",
    "conceptually it's a bit similar. For each element, we have an ID, text, and its\n",
    "embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "28cad3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import FieldSchema, DataType, CollectionSchema\n",
    "\n",
    "VECTOR_LENGTH = 768  # check the dimensionality for Silver Retriever Base (v1.1) model\n",
    "\n",
    "id_field = FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, description=\"Primary id\")\n",
    "text = FieldSchema(name=\"text\", dtype=DataType.VARCHAR, max_length=4096, description=\"Page text\")\n",
    "embedding_text = FieldSchema(\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=VECTOR_LENGTH, description=\"Embedded text\")\n",
    "\n",
    "fields = [id_field, text, embedding_text]\n",
    "\n",
    "schema = CollectionSchema(fields=fields, auto_id=True, enable_dynamic_field=True, description=\"RAG Texts collection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa4592d",
   "metadata": {},
   "source": [
    "To create a collection with the given schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4b1ba24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rag_texts_and_embeddings']\n",
      "{'collection_name': 'rag_texts_and_embeddings', 'auto_id': True, 'num_shards': 1, 'description': 'RAG Texts collection', 'fields': [{'field_id': 100, 'name': 'id', 'description': 'Primary id', 'type': <DataType.INT64: 5>, 'params': {}, 'auto_id': True, 'is_primary': True}, {'field_id': 101, 'name': 'text', 'description': 'Page text', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 4096}}, {'field_id': 102, 'name': 'embedding', 'description': 'Embedded text', 'type': <DataType.FLOAT_VECTOR: 101>, 'params': {'dim': 768}}], 'functions': [], 'aliases': [], 'collection_id': 461898704957473071, 'consistency_level': 2, 'properties': {}, 'num_partitions': 1, 'enable_dynamic_field': True, 'created_timestamp': 461898722575122437}\n"
     ]
    }
   ],
   "source": [
    "COLLECTION_NAME = \"rag_texts_and_embeddings\"\n",
    "\n",
    "milvus_client.create_collection(collection_name=COLLECTION_NAME, schema=schema)\n",
    "\n",
    "index_params = milvus_client.prepare_index_params()\n",
    "\n",
    "index_params.add_index(\n",
    "    field_name=\"embedding\",\n",
    "    index_type=\"HNSW\",\n",
    "    metric_type=\"L2\",\n",
    "    params={\"M\": 4, \"efConstruction\": 64},  # lower values for speed\n",
    ")\n",
    "\n",
    "milvus_client.create_index(collection_name=COLLECTION_NAME, index_params=index_params)\n",
    "\n",
    "# checkout our collection\n",
    "print(milvus_client.list_collections())\n",
    "\n",
    "# describe our collection\n",
    "print(milvus_client.describe_collection(COLLECTION_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90898c5c",
   "metadata": {},
   "source": [
    "Now we are able to insert documents into put database. RAG is the most useful\n",
    "when information is very specialized, niche, or otherwise probably unknown to\n",
    "the model or less popular. Let's start with [\"IAB POLSKA Przewodnik po sztucznej\n",
    "inteligencji\"](https://www.iab.org.pl/wp-content/uploads/2024/04/Przewodnik-po-sztucznej-inteligencji-2024_IAB-Polska.pdf).\n",
    "This part is inspired by [SpeakLeash](https://www.speakleash.org/) and one of\n",
    "their projects\n",
    "[Bielik-how-to-start](https://github.com/speakleash/Bielik-how-to-start?tab=readme-ov-file)\n",
    "- [Bielik_2_(4_bit)_RAG\n",
    "example](https://colab.research.google.com/drive/1ZdYsJxLVo9fW75uonXE5PCt8MBgvyktA?authuser=1).\n",
    "Bielik is the first Polish LLM, and you can also explore other tutorials for its\n",
    "usage. Let's define some constants for a start:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10c8ba9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data source and destination\n",
    "## the document origin destination from which document will be downloaded\n",
    "pdf_url = \"https://www.iab.org.pl/wp-content/uploads/2024/04/Przewodnik-po-sztucznej-inteligencji-2024_IAB-Polska.pdf\"\n",
    "\n",
    "## local destination of the document\n",
    "file_name = \"Przewodnik-po-sztucznej-inteligencji-2024_IAB-Polska.pdf\"\n",
    "\n",
    "## local destination of the processed document\n",
    "file_json = \"Przewodnik-po-sztucznej-inteligencji-2024_IAB-Polska.json\"\n",
    "\n",
    "## local destination of the embedded pages of the document\n",
    "embeddings_json = \"Przewodnik-po-sztucznej-inteligencji-2024_IAB-Polska-Embeddings.json\"\n",
    "\n",
    "## local destination of all above local required files\n",
    "data_dir = \"./data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ee7e3d",
   "metadata": {},
   "source": [
    "Let's download the document into the `data_dir` directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "910601db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "import os\n",
    "import requests\n",
    "\n",
    "\n",
    "def download_pdf_data(pdf_url: str, file_name: str) -> None:\n",
    "    response = requests.get(pdf_url, stream=True)\n",
    "    with open(os.path.join(data_dir, file_name), \"wb\") as file:\n",
    "        for block in response.iter_content(chunk_size=1024):\n",
    "            if block:\n",
    "                file.write(block)\n",
    "\n",
    "\n",
    "download_pdf_data(pdf_url, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b9c5fe",
   "metadata": {},
   "source": [
    "This is a lot of text, and in RAG we need to add specific fragments to the\n",
    "prompt. To keep things simple, and number of vectors not too large, we will\n",
    "treat each page as a separate **chunk** to vectorize and search for. Below, we\n",
    "paginate document and save each page separately into a JSON file in format\n",
    "`{\"page\": page_number, \"text\": text_of_the_page}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a71db5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "\n",
    "import fitz\n",
    "import json\n",
    "\n",
    "\n",
    "def extract_pdf_text(file_name, file_json):\n",
    "    document = fitz.open(os.path.join(data_dir, file_name))\n",
    "    pages = []\n",
    "\n",
    "    for page_num in range(len(document)):\n",
    "        page = document.load_page(page_num)\n",
    "        page_text = page.get_text()\n",
    "        pages.append({\"page_num\": page_num, \"text\": page_text})\n",
    "\n",
    "    with open(os.path.join(data_dir, file_json), \"w\") as file:\n",
    "        json.dump(pages, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "\n",
    "extract_pdf_text(file_name, file_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c28e081",
   "metadata": {},
   "source": [
    "Now we have texts, but we need vectors. We will use the model to embed text from\n",
    "each page and save the result in out collection in Milvus. It's very easy if we\n",
    "first prepare a single JSON file with all data. Its format is `{\"page\":\n",
    "page_num, \"embedding\": embedded_text}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfa9c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize data\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "def generate_embeddings(file_json, embeddings_json, model):  # noqa: F811\n",
    "    pages = []\n",
    "    with open(os.path.join(data_dir, file_json), \"r\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    for page in data:\n",
    "        pages.append(page[\"text\"])\n",
    "\n",
    "    embeddings = model.encode(pages)\n",
    "\n",
    "    embeddings_paginated = []\n",
    "    for page_num in range(len(embeddings)):\n",
    "        embeddings_paginated.append({\"page_num\": page_num, \"embedding\": embeddings[page_num].tolist()})\n",
    "\n",
    "    with open(os.path.join(data_dir, embeddings_json), \"w\") as file:\n",
    "        json.dump(embeddings_paginated, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "\n",
    "model_name = \"ipipan/silver-retriever-base-v1.1\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = SentenceTransformer(model_name, device=device)\n",
    "generate_embeddings(file_json, embeddings_json, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ca8820",
   "metadata": {},
   "source": [
    "Now we can easily insert the data into Milvus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e145347f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_embeddings(file_json, embeddings_json, client=milvus_client):\n",
    "    rows = []\n",
    "    with (\n",
    "        open(os.path.join(data_dir, file_json), \"r\") as t_f,\n",
    "        open(os.path.join(data_dir, embeddings_json), \"r\") as e_f,\n",
    "    ):\n",
    "        text_data, embedding_data = json.load(t_f), json.load(e_f)\n",
    "        text_data = list(map(lambda d: d[\"text\"], text_data))\n",
    "        embedding_data = list(map(lambda d: d[\"embedding\"], embedding_data))\n",
    "\n",
    "        for page, (text, embedding) in enumerate(zip(text_data, embedding_data)):\n",
    "            rows.append({\"text\": text, \"embedding\": embedding})\n",
    "\n",
    "    client.insert(collection_name=\"rag_texts_and_embeddings\", data=rows)\n",
    "\n",
    "\n",
    "insert_embeddings(file_json, embeddings_json)\n",
    "\n",
    "# load inserted data into memory\n",
    "milvus_client.load_collection(\"rag_texts_and_embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1515fa",
   "metadata": {},
   "source": [
    "Now let's do some semantic search!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7c4dbe78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Historia powstania\n",
      "sztucznej inteligencji\n",
      "7\n",
      "W języku potocznym „sztuczny\" oznacza to, co\n",
      "jest \n",
      "wytworem \n",
      "mającym \n",
      "naśladować \n",
      "coś\n",
      "naturalnego. W takim znaczeniu używamy\n",
      "terminu ,,sztuczny'', gdy mówimy o sztucznym\n",
      "lodowisku lub oku. Sztuczna inteligencja byłaby\n",
      "czymś (programem, maszyną) symulującym\n",
      "inteligencję naturalną, ludzką.\n",
      "Sztuczna inteligencja (AI) to obszar informatyki,\n",
      "który skupia się na tworzeniu programów\n",
      "komputerowych zdolnych do wykonywania\n",
      "zadań, które wymagają ludzkiej inteligencji. \n",
      "Te zadania obejmują rozpoznawanie wzorców,\n",
      "rozumienie języka naturalnego, podejmowanie\n",
      "decyzji, uczenie się, planowanie i wiele innych.\n",
      "Głównym celem AI jest stworzenie systemów,\n",
      "które są zdolne do myślenia i podejmowania\n",
      "decyzji na sposób przypominający ludzki.\n",
      "Historia sztucznej inteligencji sięga lat 50. \n",
      "XX wieku, kiedy to powstały pierwsze koncepcje\n",
      "i modele tego, co mogłoby stać się sztuczną\n",
      "inteligencją. Jednym z pionierów był Alan\n",
      "Turing, który sformułował test Turinga, mający\n",
      "na \n",
      "celu \n",
      "ocenę \n",
      "zdolności \n",
      "maszyny \n",
      "do\n",
      "inteligentnego \n",
      "zachowania \n",
      "na \n",
      "poziomie\n",
      "ludzkim. Jednakże dopiero w latach 80. i 90.\n",
      "nastąpił \n",
      "prawdziwy \n",
      "przełom \n",
      "w \n",
      "dziedzinie\n",
      "sztucznej \n",
      "inteligencji \n",
      "dzięki \n",
      "postępowi \n",
      "w\n",
      "dziedzinie algorytmów uczenia maszynowego.\n",
      "W wypadku sztucznej inteligencji mamy na\n",
      "uwadze system, który realizowałby niektóre\n",
      "funkcje \n",
      "umysłu \n",
      "– \n",
      "czasami \n",
      "w \n",
      "sposób\n",
      "przewyższający funkcje naturalne (na przykład,\n",
      "aby był wolny od pomyłek przy liczeniu oraz\n",
      "defektów \n",
      "pamięci). \n",
      "Inteligencja \n",
      "jest \n",
      "wła-\n",
      "ściwością umysłu. \n",
      "Składa się na nią szereg umiejętności, takich jak\n",
      "zdolność do komunikowania, rozwiązywania\n",
      "problemów, uczenia się i dostosowywania do\n",
      "sytuacji. \n",
      "Istotna \n",
      "jest \n",
      "jednak \n",
      "umiejętność\n",
      "rozumowania.\n",
      "Współczesne systemy sztucznej inteligencji są\n",
      "inteligentne tylko w ograniczonym obszarze. \n",
      "Na przykład komputer potrafi grać w szachy w\n",
      "taki \n",
      "sposób, \n",
      "że \n",
      "wygrywa \n",
      "z \n",
      "szachowym\n",
      "arcymistrzem. W 1996 r. Deep Blue wygrał jedną\n",
      "partię \n",
      "szachów \n",
      "z \n",
      "Garry \n",
      "Kasparowem,\n",
      "przegrywając cały mecz wynikiem 4:2 (przy\n",
      "dwóch remisach).\n",
      "Później Deep Blue został ulepszony i nie-\n",
      "oficjalnie \n",
      "nazwany \n",
      "„Deeper \n",
      "Blue\". \n",
      "Zagrał\n",
      "ponownie z Kasparowem w maju 1997 roku.\n",
      "Mecz \n",
      "skończył \n",
      "się \n",
      "wynikiem \n",
      "3½:2½ \n",
      "dla\n",
      "komputera. W ten sposób Deep Blue stał się\n",
      "pierwszym systemem komputerowym, który\n",
      "wygrał z aktualnym mistrzem świata w meczu\n",
      "ze standardową kontrolą czasu.\n",
      "Źródło: Midjourney – obraz wygenerowany przez AI\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# search\n",
    "def search(model, query, client=milvus_client):\n",
    "    embedded_query = model.encode(query).tolist()\n",
    "    result = client.search(\n",
    "        collection_name=\"rag_texts_and_embeddings\",\n",
    "        data=[embedded_query],\n",
    "        limit=1,\n",
    "        search_params={\"metric_type\": \"L2\"},\n",
    "        output_fields=[\"text\"],\n",
    "    )\n",
    "    return result\n",
    "\n",
    "\n",
    "result = search(model, query=\"Czym jest sztuczna inteligencja\")\n",
    "print(result[0][0][\"entity\"][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee9d48c",
   "metadata": {},
   "source": [
    "However, **this is not yet RAG!**. This is just searching through our\n",
    "embeddings, without any LLM or generation. Many companies rely on external LLMs\n",
    "used via API, due to easy setup, good scalability, and low cost. We will follow\n",
    "this trend here and use Google Gemini API to generate answer with RAG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364e8870",
   "metadata": {},
   "source": [
    "Let's prepare the function that will call Google API and generate our response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "85fa0336",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "\n",
    "GEMINI_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "gemini_client = genai.Client(api_key=GEMINI_KEY)\n",
    "\n",
    "MODEL = \"gemini-2.0-flash\"\n",
    "\n",
    "\n",
    "def generate_response(prompt: str):\n",
    "    try:\n",
    "        # Send request to Gemini 2.0 Flash API and get the response\n",
    "        response = gemini_client.models.generate_content(\n",
    "            model=MODEL,\n",
    "            contents=prompt,\n",
    "        )\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating response: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6032a09",
   "metadata": {},
   "source": [
    "Now we can fully integrate everything into a RAG system. Fill the function below\n",
    "that will augment the prompt with knowledge from Milvus, and then use the LLM to\n",
    "generate an answer based on that context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "05245400",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(context: str, query: str) -> str:\n",
    "    prompt = f\"\"\"You will be given a Question and a Context to help answer it.\n",
    "\n",
    "Your process must be:\n",
    "\n",
    "1. First, analyze the Context and extract the key pieces of information directly\n",
    "   relevant to the Question.\n",
    "\n",
    "2. Second, use only these extracted facts to synthesize a final answer.\n",
    "\n",
    "3. If no relevant information is found in the Context, respond with: \"The\n",
    "   provided context does not contain information to answer this question.\"\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def rag(model, query: str) -> str:\n",
    "    # having all prepared functions, you can combine them together and try to build your own RAG!\n",
    "    ctx = search(model, query)[0][0][\"entity\"][\"text\"]\n",
    "    prompt = build_prompt(ctx, query)\n",
    "    return generate_response(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bb6a8afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.  **Key Information Extraction:**\n",
      "\n",
      "*   AI systems can be used to create digital content that is not real (deepfake).\n",
      "*   Deepfakes are often created using someone's image (e.g., appearance, voice).\n",
      "*   Deepfakes are to be defined in the AI Act, but existing laws apply, especially regarding personal rights.\n",
      "*   Personal rights protect human integrity and are inalienable.\n",
      "*   Image (appearance, voice) is a personal right (Article 23 of the Civil Code).\n",
      "*   Using someone's image requires permission.\n",
      "*   Distributing an image without permission (e.g., in a deepfake) violates the right to that image and can result in civil liability (e.g., compensation).\n",
      "*   Deepfakes can violate other personal rights (e.g., good name) or constitute a crime (e.g., defamation, Article 212 §1 or §2 of the Criminal Code).\n",
      "*   There are limited exceptions for disseminating an image without permission, such as for a publicly known person in connection with their professional duties (Article 81 of the Copyright Act), but these should be used cautiously in the case of deepfakes.\n",
      "*   These exceptions are not adapted to image modification.\n",
      "\n",
      "2.  **Synthesized Answer:**\n",
      "\n",
      "The generation of deepfakes is regulated by existing laws, especially those concerning personal rights. Using someone's image (appearance, voice) to create a deepfake without permission violates their personal rights and can result in civil liability, such as the obligation to pay compensation. Deepfakes can also violate other personal rights or constitute a crime. While there are exceptions for using images of public figures, they should be applied cautiously to deepfakes, as they do not allow for image modification. The AI Act will define deepfakes in the future.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(rag(model, \"Jak wyglądają przepisy związane z generacją deepfake'ów?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fab6c638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided context does not contain information to answer this question.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(rag(model, \"Czy dzisiaj będzie padać?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops-course-agh-lab-08",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

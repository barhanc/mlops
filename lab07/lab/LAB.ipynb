{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a071d476",
   "metadata": {
    "id": "a071d476"
   },
   "source": [
    "# Lab 7 - Model Optimization for Inference\n",
    "\n",
    "In this lab, we will focus on optimizing neural network models for faster inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2c70b7",
   "metadata": {
    "id": "db2c70b7"
   },
   "source": [
    "### Exercise 1 (3 points)\n",
    "\n",
    "1. Load the `sentence-transformers/multi-qa-mpnet-base-cos-v1` model and\n",
    "   tokenizer. Use the `AutoModel` and `AutoTokenizer` classes from `tranformers`\n",
    "   library.\n",
    "2. Create a sample input text and tokenize it (padding, truncation,\n",
    "   `return_tensors=\"pt\"`).\n",
    "3. Measure the inference time of the model in various inference modes (average\n",
    "   time over 100 runs):\n",
    "   - no optimizations (simple PyTorch)\n",
    "   - `model.eval()`\n",
    "   - `model.eval()` and `no_grad()`\n",
    "   - `model.eval()` and `inference_mode()`\n",
    "4. Compare the speedup of options 2, 3, and 4 over the pure PyTorch. To\n",
    "   calculate speedup, divide the PyTorch time by the current time.\n",
    "\n",
    "In general, the time should decrease for subsequent options. If\n",
    "`inference_mode()` is slower than `no_grad()`, it may be due some not supported\n",
    "operations in the model, so `no_grad()` is preferred in such cases. But when\n",
    "models contain many operations and overhead with autograd is significant,\n",
    "`inference_mode()` should be faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0eef521",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313,
     "referenced_widgets": [
      "429aaf8dbd934814b802913fb4e3066f",
      "b5224f907dc54c64a33c0586146af77f",
      "42728e8e81c14033856f6a0a231d6b79",
      "37cc8a73253645d68279fc8d4bf0e40d",
      "f4c482a92e98495a9fcf618dba5ec3bb",
      "7ec1a945827d415ea129cbfb5f45cd82",
      "e5e485516669436692a347d011ea48c0",
      "16fc6b08d81f4efb9b801e36648116f3",
      "a2329144c6ff4aafa83bb64b33168275",
      "b5ce948d733c468983cc7d55792a94ec",
      "4bc6bc1a16304450a9688abd32130c6d",
      "233e0dc002be494eac2a28a54bcfb2bb",
      "3d727f2e1b3e4ade9a6298a638a302ca",
      "d1cdb4f8e0ba405abd5de09fb160520a",
      "1828bd90f2d14678a991153bccefafc4",
      "983f009546504b18b956cee509b942e0",
      "60762ad583ca47e387d9e4781759143d",
      "04bf5ab2c7404535ad321b869df3f3cc",
      "c30fead6b2494c458c7b7549d858cd4b",
      "bdf8f381f4ef4994b873c18011a8ecb3",
      "e7ce1926a79b4f60a8c8925e2e4432f1",
      "652ea3d0502b442bb7e8e91c7c895071",
      "e1a1706bb77442e49ad0517936fa5818",
      "8e589661fd3b4b0a9ab92bd5dae35a7f",
      "0d0cd91aac7949f0b291664b921b4e8b",
      "bc3a07764ea6431599b924d4f5c609de",
      "09b2127e6a8c43f8926c363c61f28dfa",
      "4a0008f6a3dc4930aaeb1c88bfb85bfb",
      "4c0d15c8d57a4ed38ced42b789c109e2",
      "b88ccd7db1b0493089b7b837328afc5a",
      "d97af30758c441ab96ddb35c49291f21",
      "5ed0a4fce8844e1ea4259d9684a44f15",
      "10b533b56ac243f0ab40eb2c0c42b7a5",
      "ca914db9a7f0442ab8509f62a6335560",
      "1196c9716c69474186ea5a335ba690c0",
      "c62e35a6967042939d136dc26340233f",
      "5bb6bd1c537248d09133c917aeceba52",
      "35d00387f5914931889b62c9b77a446f",
      "df745ef617e441608d315634fbe7b247",
      "8f5671dc51334ed3adbdf268e37cc4f0",
      "a984c91de10742da9c631338e9f41606",
      "cf839ec79f6a4bdb87de89b0917769b9",
      "271247d990b34f18a70137c43b8e7a24",
      "5fec3be05b14489a9a55fc85be26614d",
      "0e0d43ad4e6f463cba5565f77ae095a6",
      "9cb15d99887949d68c626d445bde189a",
      "0a99072af54943a9a6bf8d028e14121d",
      "86c2ea482efc42e5b6edbb2e12657423",
      "04a707db4d654136b851c6ec4b4839a0",
      "2fc3a76645194a80949780508040287a",
      "38d7d2b14ecf4b3fb331e0a318b0d601",
      "fac34f4d31cf45fcad18653021660aba",
      "bdae6e3a3ac14c2d963adcc7d11cd516",
      "c973c59c00c84c259ea822f054b9a917",
      "080a9bbcc15244cfb7e55103ad42dc49",
      "5f37935dc84649789827c4ea0c328627",
      "05f8e2a6ad2b494ba42792d9a6827330",
      "6737c84875594808b0b81f3c6524c01f",
      "13a35dc6ec394a05899646a628948dac",
      "30e6a73fa76e42019d3bdddb8d9f0cba",
      "fff0110f08b64c01bade4530d7a8474f",
      "f27623b40b514208837319b8be53e115",
      "892947791ae7421bb89f1983144d6823",
      "0fc396719f9e4c28be7a3fac1548c0da",
      "2df78c946008490eb1084b0338ecf7eb",
      "5fd65cc7681848bebf4e2ccd98c84380"
     ]
    },
    "id": "e0eef521",
    "outputId": "bd53ac34-e40b-45c2-867f-d63bc1374a37"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/multi-qa-mpnet-base-cos-v1\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/multi-qa-mpnet-base-cos-v1\")\n",
    "\n",
    "prompt = \"This is a sample prompt for inference time measurements.\"\n",
    "inputs = tokenizer(prompt, truncation=True, padding=\"max_length\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b45574",
   "metadata": {
    "id": "35b45574"
   },
   "source": [
    "- no optimizations (simple PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "36a26d14",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "id": "36a26d14",
    "outputId": "665b3a3e-8cdb-441e-b3ac-55ec5ffaa44f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280.540 ms/run | Speedup: 1.00x\n"
     ]
    }
   ],
   "source": [
    "from timeit import timeit\n",
    "\n",
    "model.train()\n",
    "t1 = timeit(lambda: model(**inputs), number=100)\n",
    "print(f\"{t1 * 10:.3f} ms/run | Speedup: {t1 / t1:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0292bb",
   "metadata": {
    "id": "2e0292bb"
   },
   "source": [
    "- `model.eval()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6d9d3e47",
   "metadata": {
    "id": "6d9d3e47",
    "outputId": "d6d3cf1e-44ee-40a5-b9ab-dfe6067e287d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231.350 ms/run | Speedup: 1.21x\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "t2 = timeit(lambda: model(**inputs), number=100)\n",
    "print(f\"{t2 * 10:.3f} ms/run | Speedup: {t1 / t2:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5b4129",
   "metadata": {
    "id": "5f5b4129"
   },
   "source": [
    "- `model.eval()` and `no_grad()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89ec94b",
   "metadata": {
    "id": "a89ec94b",
    "outputId": "63e65117-0b1e-48a5-cf33-43cf04ac1a56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210.753 ms/run | Speedup: 1.33x\n"
     ]
    }
   ],
   "source": [
    "def predict_eval_nograd(inputs):\n",
    "    with torch.no_grad():\n",
    "        return model(**inputs)\n",
    "\n",
    "\n",
    "t3 = timeit(lambda: predict_eval_nograd(inputs), number=100)\n",
    "print(f\"{t3 * 10:.3f} ms/run | Speedup: {t1 / t3:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ee0c08",
   "metadata": {
    "id": "45ee0c08"
   },
   "source": [
    "- `model.eval()` and `inference_mode()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7b3689",
   "metadata": {
    "id": "9e7b3689",
    "outputId": "d6451ab5-ef00-4249-b9b4-3743ccea8a0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "213.607 ms/run | Speedup: 1.31x\n"
     ]
    }
   ],
   "source": [
    "def predict_eval_inference(inputs):\n",
    "    with torch.inference_mode():\n",
    "        return model(**inputs)\n",
    "\n",
    "\n",
    "t4 = timeit(lambda: predict_eval_inference(inputs), number=100)\n",
    "print(f\"{t4 * 10:.3f} ms/run | Speedup: {t1 / t4:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59277440",
   "metadata": {
    "id": "59277440"
   },
   "source": [
    "### Exercise 2 (2 points)\n",
    "\n",
    "In this exercise, we will verify the gains from model compilation with\n",
    "`torch.compile()`.\n",
    "\n",
    "1. Compile the model using `torch.compile()` after switching it to evaluation\n",
    "   mode, and warm-up the model by running a single inference call. Measure this\n",
    "   compilation + warm-up time (just once).\n",
    "2. Measure the inference time (average of 100 runs) of the compiled model in\n",
    "   inference mode.\n",
    "3. Calculate the speedup, and compare results with those from the previous\n",
    "   exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70262871",
   "metadata": {
    "id": "70262871",
    "outputId": "47ed6de6-ae0a-48de-cba5-2adbfc599d64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compilation time: 7.65 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "t = time.perf_counter()\n",
    "\n",
    "model.eval()\n",
    "model_compiled = torch.compile(model)\n",
    "_ = model_compiled(**inputs)  # warm-up\n",
    "\n",
    "t = time.perf_counter() - t\n",
    "print(f\"Compilation time: {t:.2f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9942539a",
   "metadata": {
    "id": "9942539a",
    "outputId": "fe9d06eb-9fbb-46d1-d3c3-734b28d9768e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227.483 ms/run | Speedup: 1.16x\n"
     ]
    }
   ],
   "source": [
    "def predict_compiled(inputs):\n",
    "    with torch.inference_mode():\n",
    "        return model_compiled(**inputs)\n",
    "\n",
    "\n",
    "t5 = timeit(lambda: predict_compiled(inputs), number=100)\n",
    "print(f\"{t5 * 10:.3f} ms/run | Speedup: {t1 / t5:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef238ce",
   "metadata": {
    "id": "3ef238ce"
   },
   "source": [
    "### Exercise 3 (3 points)\n",
    "\n",
    "We will perform a dynamic quantization for our model, which is very simple\n",
    "operationally to use with PyTorch. It provides the\n",
    "`torch.ao.quantization.quantize_dynamic()` function, to which we pass the model\n",
    "and a list of layer types that we want to quantize. In the case of transformers,\n",
    "those are primarily the linear layers, which contain the majority of weights and\n",
    "perform most computations.\n",
    "\n",
    "1. Ensure the model is on the CPU.\n",
    "2. Quantize the model with `torch.ao.quantization.quantize_dynamic()`, setting\n",
    "   the target weight to `torch.qint8` and layers to a single-element set with\n",
    "   `nn.Linear`.\n",
    "3. Save the model to a new variable (e.g. `model_quantized`), and print it to\n",
    "   verify that linear layers have been quantized properly (i.e.\n",
    "   `DynamicQuantizedLinear` instead of `Linear`).\n",
    "4. Save both models to disk (`state_dict` for both) and compare the file sizes\n",
    "   (e.g. `os.path.getsize()`).\n",
    "5. Compare the inference speed and speedup on CPU for original and quantized\n",
    "   models (again, average of 100 runs).\n",
    "6. Display the comparison. Do you think that quantization is helpful in this\n",
    "   case?\n",
    "\n",
    "Typically, we would observe the reduction in model size up to 4x and speedup of\n",
    "1.5-2x, depending on the model type and what parameters exactly are quantized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cd8042",
   "metadata": {
    "id": "29cd8042",
    "outputId": "fdc1d797-babe-435e-dd23-b7a2853ac02e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a69a5e4",
   "metadata": {
    "id": "0a69a5e4",
    "outputId": "c47bd11b-ab14-4ac9-c0a4-c878967579df"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MPNetModel(\n",
       "  (embeddings): MPNetEmbeddings(\n",
       "    (word_embeddings): Embedding(30527, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): MPNetEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x MPNetLayer(\n",
       "        (attention): MPNetAttention(\n",
       "          (attn): MPNetSelfAttention(\n",
       "            (q): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (k): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (v): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (o): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (intermediate): MPNetIntermediate(\n",
       "          (dense): DynamicQuantizedLinear(in_features=768, out_features=3072, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): MPNetOutput(\n",
       "          (dense): DynamicQuantizedLinear(in_features=3072, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (relative_attention_bias): Embedding(32, 12)\n",
       "  )\n",
       "  (pooler): MPNetPooler(\n",
       "    (dense): DynamicQuantizedLinear(in_features=768, out_features=768, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_quantized = torch.ao.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\n",
    "model_quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e14330b",
   "metadata": {
    "id": "7e14330b",
    "outputId": "db3370ff-2219-40d1-90fc-38850c217f1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla   model size: 417.72 MB\n",
      "Quantized model size: 173.10 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "torch.save(model.state_dict(), \"models/model.wts\")\n",
    "torch.save(model_quantized.state_dict(), \"models/model_quantized.wts\")\n",
    "\n",
    "print(f\"Vanilla   model size: {os.path.getsize('models/model.wts') / 1024 / 1024:.2f} MB\")\n",
    "print(f\"Quantized model size: {os.path.getsize('models/model_quantized.wts') / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706be867",
   "metadata": {
    "id": "706be867",
    "outputId": "a1db2346-5e4a-42d2-af29-094736d402d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267.568 ms/run | Speedup: 1.00x\n"
     ]
    }
   ],
   "source": [
    "def predict_vanilla(inputs):\n",
    "    with torch.inference_mode():\n",
    "        return model(**inputs)\n",
    "\n",
    "\n",
    "t_vanilla = timeit(lambda: predict_vanilla(inputs), number=100)\n",
    "print(f\"{t_vanilla * 10:.3f} ms/run | Speedup: {t_vanilla / t_vanilla:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce42cd5",
   "metadata": {
    "id": "bce42cd5",
    "outputId": "24c9e732-6983-4376-f78a-3ff5ab471d34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208.267 ms/run | Speedup: 1.16x\n"
     ]
    }
   ],
   "source": [
    "def predict_quantized(inputs):\n",
    "    with torch.inference_mode():\n",
    "        return model_quantized(**inputs)\n",
    "\n",
    "\n",
    "t_quantized = timeit(lambda: predict_quantized(inputs), number=100)\n",
    "print(f\"{t_quantized * 10:.3f} ms/run | Speedup: {t_vanilla / t_quantized:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3606f57f",
   "metadata": {},
   "source": [
    "Quantization greatly reduces the model size (~2x) but only slightly speeds-up\n",
    "the inference. It is definietly worth it, though."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8942750a",
   "metadata": {
    "id": "8942750a"
   },
   "source": [
    "### Exercise 4 (2 points)\n",
    "\n",
    "1. Compare inference time of:\n",
    "   - `torch.compile()` with default settings\n",
    "   - `torch.compile()` with `mode=\"max-autotune\"`\n",
    "   - `torch.compile()` with `mode=\"max-autotune-no-cudagraphs\"`\n",
    "2. Report the average time of 100 runs and speedup of the latter two modes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97f1f9c5",
   "metadata": {
    "id": "97f1f9c5"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "model = model.to(device)\n",
    "inputs_gpu = {k: v.to(device) for k, v in inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "801cfd5e",
   "metadata": {
    "id": "801cfd5e"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "compiled_model_with_cudagraphs = torch.compile(model, mode=\"max-autotune\")\n",
    "compiled_model_dynamic = torch.compile(model, mode=\"max-autotune-no-cudagraphs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad131ec",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ad131ec",
    "outputId": "2ac4aaf0-8c05-4630-b7aa-1d7e4e5f8861"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47.856 ms/run | Speedup: 1.00x\n"
     ]
    }
   ],
   "source": [
    "def predict_gpu(inputs):\n",
    "    with torch.inference_mode():\n",
    "        return model(**inputs)\n",
    "\n",
    "\n",
    "t_default = timeit(lambda: predict_gpu(inputs_gpu), number=100)\n",
    "print(f\"{t_default * 10:.3f} ms/run | Speedup: {t_default / t_default:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f97a6ce",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8f97a6ce",
    "outputId": "e58af3bb-e7a5-427b-fdda-2697defb1a5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.624 ms/run | Speedup: 1.87x\n"
     ]
    }
   ],
   "source": [
    "def predict_gpu_cudagraph(inputs):\n",
    "    with torch.inference_mode():\n",
    "        return compiled_model_with_cudagraphs(**inputs)\n",
    "\n",
    "\n",
    "t_with_cudagraphs = timeit(lambda: predict_gpu_cudagraph(inputs_gpu), number=100)\n",
    "print(f\"{t_with_cudagraphs * 10:.3f} ms/run | Speedup: {t_default / t_with_cudagraphs:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa9348e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "caa9348e",
    "outputId": "76c658e3-8c03-4b60-fdce-d1a74bb8dfde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.947 ms/run | Speedup: 1.78x\n"
     ]
    }
   ],
   "source": [
    "def predict_gpu_dynamic(inputs):\n",
    "    with torch.inference_mode():\n",
    "        return compiled_model_dynamic(**inputs)\n",
    "\n",
    "\n",
    "t_dynamic = timeit(lambda: predict_gpu_dynamic(inputs_gpu), number=100)\n",
    "print(f\"{t_dynamic * 10:.3f} ms/run | Speedup: {t_default / t_dynamic:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759e5651",
   "metadata": {
    "id": "759e5651"
   },
   "source": [
    "### Exercise 5 (2 points)\n",
    "\n",
    "1. Check if your GPU supports Tensor Cores (capability >= (7,0)). If not, switch\n",
    "   to Google Colab with GPU runtime.\n",
    "2. Measure inference time with:\n",
    "   - full precision (`float32`)\n",
    "   - manual half-precision (`float16`)\n",
    "   - automatic mixed precision (`torch.autocast`)\n",
    "3. Compare time and speedup. Which variant would you use in practice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb668be0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bb668be0",
    "outputId": "45e8d72a-d7c4-4af8-9d33-e6d27b5ab995"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA device capability: (7, 5)\n",
      "Tensor Cores available: fast float16 supported.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "capability = torch.cuda.get_device_capability()\n",
    "print(f\"CUDA device capability: {capability}\")\n",
    "\n",
    "# Tensor Cores are available on NVidia GPUs with CUDA >= 7 (e.g. Volta, Turing, Ampere, Hopper)\n",
    "if capability >= (7, 0):\n",
    "    print(\"Tensor Cores available: fast float16 supported.\")\n",
    "else:\n",
    "    print(\"Tensor Cores not available: float16 may be slow or unsupported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca65adc9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ca65adc9",
    "outputId": "a3b9a415-2df0-4ce2-b2d5-fbed28cb5326"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.646 ms/run | Speedup: 1.00x\n"
     ]
    }
   ],
   "source": [
    "def predict_fp32(inputs):\n",
    "    with torch.inference_mode():\n",
    "        return model(**inputs)\n",
    "\n",
    "\n",
    "t_fp32 = timeit(lambda: predict_fp32(inputs_gpu), number=100)\n",
    "print(f\"{t_fp32 * 10:.3f} ms/run | Speedup: {t_fp32 / t_fp32:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd133697",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bd133697",
    "outputId": "4b208155-33bf-4915-8c4c-8a8637157400"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.267 ms/run | Speedup: 1.95x\n"
     ]
    }
   ],
   "source": [
    "model_half = model.half().to(\"cuda\")\n",
    "\n",
    "\n",
    "def predict_fp16(inputs):\n",
    "    with torch.inference_mode():\n",
    "        return model_half(**inputs)\n",
    "\n",
    "\n",
    "t_fp16 = timeit(lambda: predict_fp16(inputs_gpu), number=100)\n",
    "print(f\"{t_fp16 * 10:.3f} ms/run | Speedup: {t_fp32 / t_fp16:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19b0bb5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a19b0bb5",
    "outputId": "ef3d6b93-74ab-4e15-c948-40d95b204c88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.402 ms/run | Speedup: 2.34x\n"
     ]
    }
   ],
   "source": [
    "def predict_autocast(inputs):\n",
    "    with torch.inference_mode(), torch.autocast(device, dtype=torch.float16):\n",
    "        return model(**inputs)\n",
    "\n",
    "\n",
    "t_auto = timeit(lambda: predict_autocast(inputs_gpu), number=100)\n",
    "print(f\"{t_auto * 10:.3f} ms/run | Speedup: {t_fp32 / t_auto:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2529de2f",
   "metadata": {},
   "source": [
    "Torch autocast seems like the best option, it is both the fastest and easies to\n",
    "use (just add the context manager)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6QVtAcAYqlqY",
   "metadata": {
    "id": "6QVtAcAYqlqY"
   },
   "source": [
    "### Exercise 6 (3 points)\n",
    "\n",
    "1. Measure cold start time (including session creation) of the ONNX model using online and offline optimization modes\n",
    "   on CPU.\n",
    "2. Measure inference time of the ONNX model on CPU using both optimization modes.\n",
    "3. Prepare deployment Docker images:\n",
    "   - build two images, for a) compiled PyTorch model b) ONNX model with ONNX Runtime\n",
    "   - select the best model in both cases in terms of the inference time\n",
    "   - install a minimal set of requirements in both cases, e.g. do not install PyTorch for ONNX image\n",
    "4. Compare for those apps:\n",
    "   - Docker container sizes\n",
    "   - response time (average of 100 requests)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "gp3s6ncwqrdi",
   "metadata": {
    "id": "gp3s6ncwqrdi"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.onnx\n",
    "\n",
    "# Put the model in eval mode and move to CPU\n",
    "model_cpu = model.eval().cpu()\n",
    "\n",
    "# Example input for tracking (for onnx export)\n",
    "sample_input = tokenizer(\n",
    "    \"This is a sample input text for ONNX export.\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "# Export to ONNX format\n",
    "torch.onnx.export(\n",
    "    model_cpu,\n",
    "    (sample_input[\"input_ids\"], sample_input[\"attention_mask\"]),\n",
    "    \"model.onnx\",\n",
    "    opset_version=17,\n",
    "    input_names=[\"input_ids\", \"attention_mask\"],\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={\n",
    "        \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"attention_mask\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
    "        \"output\": {0: \"batch_size\"},\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a504652",
   "metadata": {},
   "source": [
    "* online mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b2aCapb_qsjF",
   "metadata": {
    "id": "b2aCapb_qsjF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cold Start Time: 344.02 ms\n",
      "Inference Time:  14.17 ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import onnxruntime as ort\n",
    "\n",
    "# Prepare input data\n",
    "sample_input = tokenizer(\n",
    "    \"This is a sample input text for ONNX inference.\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"np\",\n",
    ")\n",
    "\n",
    "# Create input dictionary, in same format as during export\n",
    "inputs_onnx = {\n",
    "    \"input_ids\": sample_input[\"input_ids\"],\n",
    "    \"attention_mask\": sample_input[\"attention_mask\"],\n",
    "}\n",
    "\n",
    "t = time.perf_counter()\n",
    "\n",
    "options = ort.SessionOptions()\n",
    "options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "ort_session = ort.InferenceSession(\"model.onnx\", sess_options=options, providers=[\"CPUExecutionProvider\"])\n",
    "\n",
    "cold_start_time = time.perf_counter() - t\n",
    "print(f\"Cold Start Time: {cold_start_time * 1000:.2f} ms\")\n",
    "\n",
    "t = time.perf_counter()\n",
    "_ = ort_session.run(None, inputs_onnx)\n",
    "inference_time = time.perf_counter() - t\n",
    "print(f\"Inference Time:  {inference_time * 1000:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8683a793",
   "metadata": {},
   "source": [
    "* offline mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c574eebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cold Start Time: 259.44 ms\n",
      "Inference Time:  13.09 ms\n"
     ]
    }
   ],
   "source": [
    "opt_options = ort.SessionOptions()\n",
    "opt_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_EXTENDED\n",
    "opt_options.optimized_model_filepath = \"model_optimized.onnx\"\n",
    "ort.InferenceSession(\"model.onnx\", sess_options=opt_options, providers=[\"CPUExecutionProvider\"])\n",
    "\n",
    "t = time.perf_counter()\n",
    "\n",
    "options = ort.SessionOptions()\n",
    "options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_DISABLE_ALL\n",
    "session = ort.InferenceSession(\"model_optimized.onnx\", sess_options=options, providers=[\"CPUExecutionProvider\"])\n",
    "\n",
    "cold_start_time = time.perf_counter() - t\n",
    "print(f\"Cold Start Time: {cold_start_time * 1000:.2f} ms\")\n",
    "\n",
    "t = time.perf_counter()\n",
    "_ = session.run(None, inputs_onnx)\n",
    "inference_time = time.perf_counter() - t\n",
    "print(f\"Inference Time:  {inference_time * 1000:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f784b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "\n",
    "\n",
    "def benchmark_endpoint(name, url, payload, n=100):\n",
    "    latencies = []\n",
    "    requests.post(url, json=payload)\n",
    "\n",
    "    for _ in trange(n, desc=f\"Benchmarking {name}...\"):\n",
    "        t = time.perf_counter()\n",
    "        requests.post(url, json=payload)\n",
    "        t = time.perf_counter() - t\n",
    "        latencies.append(t * 1000)\n",
    "\n",
    "    print(f\"{name} Avg Response Time: {np.mean(latencies):.2f} ms ({n} runs)\")\n",
    "\n",
    "\n",
    "payload = {\"text\": \"Some text...\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7ff8ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID   IMAGE                 COMMAND                  CREATED          STATUS          PORTS                                         NAMES                   SIZE\n",
      "b642eaa573e6   torch_app-torch-app   \"uv run uvicorn main…\"   3 minutes ago    Up 3 minutes    0.0.0.0:8001->8001/tcp, [::]:8001->8001/tcp   torch_app-torch-app-1   641MB (virtual 2.25GB)\n",
      "6b4ffff7553c   onnx_app-onnx-app     \"uv run uvicorn main…\"   20 minutes ago   Up 20 minutes   0.0.0.0:8000->8000/tcp, [::]:8000->8000/tcp   onnx_app-onnx-app-1     17.7MB (virtual 896MB)\n"
     ]
    }
   ],
   "source": [
    "!docker ps --size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c870fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Benchmarking ONNX Runtime...: 100%|██████████| 100/100 [00:01<00:00, 59.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX Runtime Avg Response Time: 16.64 ms (100 runs)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "benchmark_endpoint(\"ONNX Runtime\", \"http://localhost:8000/predict\", payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0b1f109",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Benchmarking PyTorch CPU...: 100%|██████████| 100/100 [00:18<00:00,  5.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch CPU Avg Response Time: 187.15 ms (100 runs)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "benchmark_endpoint(\"PyTorch CPU\", \"http://localhost:8001/predict\", payload)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "mlops-course-agh-lab-07",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
